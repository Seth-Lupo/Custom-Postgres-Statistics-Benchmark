"""
PostgreSQL Statistics Estimator - Experiment Runner Service

This module provides the main experiment execution orchestration functionality.
It coordinates between all the other services to execute complete experiments
while maintaining separation of concerns and modularity.

Classes:
    ExperimentRunner: Main experiment orchestration service

Author: Generated by Assistant
Created: 2024
"""

import re
import statistics
import traceback
from typing import List, Tuple, Callable, Dict, Any
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError
from sqlmodel import Session
from datetime import datetime

from ..models import Experiment, Trial
from ..src.base import StatsSource, StatsSourceConfig
from ..logging_config import experiment_logger, stats_logger, stats_source_logger
from ..database import create_database, drop_database, load_dump, get_db_session

from .progress_tracker import ProgressTracker
from .statistics_capture import StatisticsCapture
from .trial_executor import TrialExecutor, QueryExecutionError
from .experiment_validator import ExperimentValidator, ValidationError


class ExperimentError(Exception):
    """Base class for experiment-related errors."""
    pass


class StatsApplicationError(ExperimentError):
    """Error during statistics application."""
    pass


class ExperimentRunner:
    """
    Main experiment runner service that orchestrates experiment execution.
    
    This service coordinates:
    - Statistics source discovery and management
    - Experiment validation and setup
    - Database creation and management
    - Trial execution orchestration
    - Progress tracking and logging
    - Results aggregation and storage
    """
    
    def __init__(self):
        """Initialize the experiment runner with all necessary services."""
        self.src = {}
        self._discover_stats_sources()
        
        # Initialize services
        self.progress_tracker = ProgressTracker()
        self.statistics_capture = StatisticsCapture()
        self.trial_executor = TrialExecutor()
        self.validator = ExperimentValidator(self.src)
        
    def _discover_stats_sources(self) -> None:
        """Discover all available StatsSource subclasses."""
        for subclass in StatsSource.__subclasses__():
            # Convert class name to snake_case key
            class_name = subclass.__name__
            if class_name.endswith('StatsSource'):
                class_name = class_name[:-11]
            
            snake_case = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', class_name)
            key = re.sub('([a-z0-9])([A-Z])', r'\1_\2', snake_case).lower()
            
            self.src[key] = subclass
            
        experiment_logger.info(f"Discovered {len(self.src)} statistics sources")
    
    def get_available_src(self) -> List[Tuple[str, str]]:
        """
        Get list of available statistics sources.
        
        Returns:
            List of (key, display_name) tuples
        """
        return [(key, source().name()) for key, source in self.src.items()]
    
    def get_available_configs(self, stats_source: str) -> List[Tuple[str, str]]:
        """
        Get list of available configurations for a stats source.
        
        Args:
            stats_source: Statistics source key
            
        Returns:
            List of (config_name, display_name) tuples
        """
        if stats_source not in self.src:
            return []
        
        source_class = self.src[stats_source]
        instance = source_class()
        return instance.get_available_configs()
    
    def run_experiment(self, session: Session, stats_source: str, config_name: str, 
                      config_yaml: str, query: str, iterations: int, 
                      stats_reset_strategy: str, transaction_handling: str, 
                      progress_callback: Callable[[str, int, int], None], 
                      dump_path: str, name: str) -> Experiment:
        """
        Run a complete experiment and return the result.
        
        Args:
            session: Database session for storing results
            stats_source: Statistics source identifier
            config_name: Named configuration (optional)
            config_yaml: Custom YAML configuration (optional)
            query: SQL query to execute
            iterations: Number of trial iterations
            stats_reset_strategy: Statistics reset strategy ('once' or 'per_trial')
            transaction_handling: Transaction handling ('rollback' or 'persist')
            progress_callback: Callback function for progress updates
            dump_path: Path to database dump file
            name: Experiment name
            
        Returns:
            Completed experiment object
            
        Raises:
            ExperimentError: If experiment execution fails
            ValidationError: If parameter validation fails
        """
        db_name = "test_database"
        
        # Initialize progress tracking
        self.progress_tracker.clear_logs()
        self.progress_tracker.set_progress_callback(progress_callback)
        
        # Validate all parameters before starting
        try:
            self.validator.validate_experiment_parameters(
                stats_source, config_name, config_yaml, query, iterations,
                stats_reset_strategy, transaction_handling, dump_path, name
            )
        except ValidationError as e:
            experiment_logger.error(f"Validation failed: {str(e)}")
            raise ExperimentError(f"Validation failed: {str(e)}") from e
        
        experiment_logger.info(f"Starting experiment '{name}' with {stats_source} stats source")
        experiment_logger.debug(f"Query: {query}")
        experiment_logger.debug(f"Iterations: {iterations}")
        experiment_logger.info(f"Stats reset strategy: {stats_reset_strategy}")  
        experiment_logger.info(f"Transaction handling: {transaction_handling}")
        experiment_logger.info(f"Using temporary database: {db_name} from dump: {dump_path}")
        
        # Setup statistics source and configuration
        stats_source_instance, original_config_yaml, config_modified, config_modified_at = \
            self._setup_stats_source_config(stats_source, config_name, config_yaml)
            
        # Store the actual YAML configuration that will be used
        actual_config_yaml = config_yaml if config_yaml else original_config_yaml
        
        # Create experiment record
        experiment = self._create_experiment_record(
            session, name, stats_source_instance, config_name, actual_config_yaml,  
            original_config_yaml, config_modified, config_modified_at, query,
            iterations, stats_reset_strategy, transaction_handling
        )
        
        # Execute the experiment
        try:
            self._execute_experiment_trials(
                session, experiment, stats_source_instance, query, iterations,
                stats_reset_strategy, transaction_handling, dump_path, db_name
            )
            
            # Calculate and store final statistics
            self._finalize_experiment_results(session, experiment)
            
            experiment_logger.info("Experiment completed successfully")
            return experiment
            
        except Exception as e:
            self._handle_experiment_failure(session, experiment, e)
            raise
            
    def _setup_stats_source_config(self, stats_source: str, config_name: str, 
                                  config_yaml: str) -> Tuple[StatsSource, str, bool, datetime]:
        """
        Setup statistics source instance and configuration.
        
        Returns:
            Tuple of (stats_source_instance, original_config_yaml, config_modified, config_modified_at)
        """
        source_class = self.src[stats_source]
        
        # Initialize configuration tracking variables
        original_config_yaml = None
        config_modified = False
        config_modified_at = None
        
        if config_yaml:
            # Use custom YAML configuration provided by user
            import yaml
            config_data = yaml.safe_load(config_yaml)
            config = StatsSourceConfig(config_data)
            stats_source_instance = source_class(config)
            experiment_logger.info(f"Using custom configuration: {config.name}")
            
            # Get original configuration for comparison
            effective_config_name = config_name or 'default'
            original_config_yaml = source_class().get_config_content(effective_config_name)
            
            # Check if configuration was actually modified
            if original_config_yaml and config_yaml.strip() != original_config_yaml.strip():
                config_modified = True
                config_modified_at = datetime.utcnow()
                experiment_logger.info("Configuration was modified from original")
            else:
                experiment_logger.info("Configuration unchanged from original")
        else:
            # Use named or default configuration
            effective_config_name = config_name or 'default'
            config = source_class().load_config(effective_config_name)
            stats_source_instance = source_class(config)
            experiment_logger.info(f"Using named/default configuration: {effective_config_name}")
            
            # Get the original YAML for the named configuration
            original_config_yaml = source_class().get_config_content(effective_config_name)
            
        return stats_source_instance, original_config_yaml, config_modified, config_modified_at
        
    def _create_experiment_record(self, session: Session, name: str, 
                                stats_source_instance: StatsSource, config_name: str,
                                actual_config_yaml: str, original_config_yaml: str,
                                config_modified: bool, config_modified_at: datetime,
                                query: str, iterations: int, stats_reset_strategy: str,
                                transaction_handling: str) -> Experiment:
        """Create and store experiment record in database."""
        try:
            self.progress_tracker.log_and_callback("Creating experiment record...", 0, iterations)
            
            experiment = Experiment(
                name=name,
                stats_source=stats_source_instance.display_name(),
                config_name=config_name or 'default',
                config_yaml=actual_config_yaml,
                original_config_yaml=original_config_yaml,
                config_modified=config_modified,
                config_modified_at=config_modified_at,
                query=query,
                iterations=iterations,
                stats_reset_strategy=stats_reset_strategy,
                transaction_handling=transaction_handling,
                exit_status="RUNNING"
            )
            
            session.add(experiment)
            session.commit()
            session.refresh(experiment)
            experiment_logger.info(f"Created experiment record with ID {experiment.id}")
            
            return experiment
            
        except SQLAlchemyError as e:
            error_msg = f"Failed to create experiment record: {str(e)}"
            experiment_logger.error(error_msg)
            experiment_logger.debug(traceback.format_exc())
            raise ExperimentError(error_msg) from e
            
    def _execute_experiment_trials(self, session: Session, experiment: Experiment,
                                 stats_source_instance: StatsSource, query: str,
                                 iterations: int, stats_reset_strategy: str,
                                 transaction_handling: str, dump_path: str, db_name: str) -> None:
        """Execute all experiment trials."""
        experiment_db_session_generator = None
        experiment_db_session = None
        
        try:
            # Setup temporary database
            self._setup_temporary_database(db_name, dump_path)
            
            # Get database session and configure it
            experiment_db_session_generator = get_db_session(db_name)
            experiment_db_session = next(experiment_db_session_generator)
            self._configure_database_session(experiment_db_session, stats_source_instance)
            
            # Apply statistics based on strategy
            if stats_reset_strategy == "once":
                self._apply_statistics_once(experiment_db_session, stats_source_instance, iterations)
                
            # Execute all trials
            self._run_all_trials(
                session, experiment, experiment_db_session, stats_source_instance,
                query, iterations, stats_reset_strategy, transaction_handling
            )
            
        finally:
            # Cleanup database resources
            if experiment_db_session_generator:
                try:
                    next(experiment_db_session_generator)
                except StopIteration:
                    pass
                    
            self.progress_tracker.log_and_callback(f"Cleaning up temporary database '{db_name}'...", iterations, iterations)
            drop_database(db_name)
            experiment_logger.info(f"Dropped temporary database: {db_name}")
            
    def _setup_temporary_database(self, db_name: str, dump_path: str) -> None:
        """Setup temporary database for experiment."""
        self.progress_tracker.log_and_callback(f"Creating temporary database '{db_name}'...", 0, 0)
        create_database(db_name)
        
        self.progress_tracker.log_and_callback(f"Loading data from '{dump_path}'...", 0, 0)
        load_dump(db_name, dump_path)
        
    def _configure_database_session(self, session: Session, stats_source_instance: StatsSource) -> None:
        """Configure database session parameters."""
        # Set session parameters for consistent execution environment
        timeout = stats_source_instance.config.get_setting('statement_timeout_ms', 0)
        session.execute(text(f"SET statement_timeout = {timeout}"))
        
        work_mem = stats_source_instance.config.get_setting('work_mem', '16MB')
        session.execute(text(f"SET work_mem = '{work_mem}'"))
        
        maint_work_mem = stats_source_instance.config.get_setting('maintenance_work_mem', '16MB')
        session.execute(text(f"SET maintenance_work_mem = '{maint_work_mem}'"))
        
        cache_size = stats_source_instance.config.get_setting('effective_cache_size', '1GB')
        session.execute(text(f"SET effective_cache_size = '{cache_size}'"))
        
        random_cost = stats_source_instance.config.get_setting('random_page_cost', 1.0)
        session.execute(text(f"SET random_page_cost = {random_cost}"))
        
        seq_cost = stats_source_instance.config.get_setting('seq_page_cost', 1.0)
        session.execute(text(f"SET seq_page_cost = {seq_cost}"))
        
        session.commit()
        
    def _apply_statistics_once(self, session: Session, stats_source_instance: StatsSource, iterations: int) -> None:
        """Apply statistics once before all trials."""
        msg = f"Applying {stats_source_instance.name()} statistics to database (once before all trials)..."
        self.progress_tracker.log_and_callback(msg, 0, iterations)
        experiment_logger.info(msg)
        
        try:
            stats_source_instance.apply_statistics(session)
            stats_logger.info(f"Successfully applied {stats_source_instance.name()} statistics")
        except Exception as e:
            error_msg = f"Failed to apply statistics: {str(e)}"
            stats_logger.error(error_msg)
            stats_logger.debug(traceback.format_exc())
            raise StatsApplicationError(error_msg) from e
            
    def _run_all_trials(self, session: Session, experiment: Experiment, 
                       experiment_db_session: Session, stats_source_instance: StatsSource,
                       query: str, iterations: int, stats_reset_strategy: str,
                       transaction_handling: str) -> None:
        """Execute all experiment trials."""
        execution_times = []
        query_plans = []
        
        for i in range(iterations):
            trial_msg = f"Running trial {i + 1}/{iterations}..."
            self.progress_tracker.log_and_callback(trial_msg, i, iterations)
            experiment_logger.info(trial_msg)
            
            try:
                # Apply statistics per trial if needed
                if stats_reset_strategy == "per_trial":
                    self._apply_statistics_per_trial(experiment_db_session, stats_source_instance, i + 1)
                    
                # Execute the trial
                execution_time, cost_estimate, query_plan = self.trial_executor.execute_trial(
                    experiment_db_session, query, transaction_handling, stats_source_instance
                )
                
                execution_times.append(execution_time)
                query_plans.append(query_plan)
                
                # Capture statistics snapshots
                pg_stats_snapshot, pg_statistic_snapshot = \
                    self.statistics_capture.capture_statistics_snapshots(experiment_db_session)
                
                # Record trial in database
                self._record_trial_result(
                    session, experiment.id, i + 1, execution_time, cost_estimate,
                    pg_stats_snapshot, pg_statistic_snapshot
                )
                
                result_msg = f"Trial {i + 1} completed: Time={execution_time:.4f}s, Cost={cost_estimate:.2f}"
                self.progress_tracker.log_and_callback(result_msg, i + 1, iterations)
                
            except Exception as e:
                error_msg = f"Error in trial {i + 1}: {str(e)}"
                experiment_logger.error(error_msg)
                self.progress_tracker.log_and_callback(f"⚠️ {error_msg}", i + 1, iterations)
                raise QueryExecutionError(error_msg) from e
                
        # Store execution times for final analysis
        experiment._execution_times = execution_times
        experiment._query_plans = query_plans
        
    def _apply_statistics_per_trial(self, session: Session, stats_source_instance: StatsSource, trial_num: int) -> None:
        """Apply statistics for a specific trial."""
        trial_stats_msg = f"Resetting and applying {stats_source_instance.name()} statistics for trial {trial_num}..."
        self.progress_tracker.log_and_callback(trial_stats_msg, trial_num - 1, 0)
        experiment_logger.info(trial_stats_msg)
        
        try:
            stats_source_instance.apply_statistics(session)
            stats_logger.info(f"Successfully applied {stats_source_instance.name()} statistics for trial {trial_num}")
        except Exception as e:
            error_msg = f"Failed to apply statistics for trial {trial_num}: {str(e)}"
            stats_logger.error(error_msg)
            stats_logger.debug(traceback.format_exc())
            raise StatsApplicationError(error_msg) from e
            
    def _record_trial_result(self, session: Session, experiment_id: int, run_index: int,
                           execution_time: float, cost_estimate: float,
                           pg_stats_snapshot: str, pg_statistic_snapshot: str) -> None:
        """Record trial result in database."""
        trial = Trial(
            experiment_id=experiment_id,
            run_index=run_index,
            execution_time=execution_time,
            cost_estimate=cost_estimate,
            pg_stats_snapshot=pg_stats_snapshot,
            pg_statistic_snapshot=pg_statistic_snapshot
        )
        session.add(trial)
        session.commit()
        
    def _finalize_experiment_results(self, session: Session, experiment: Experiment) -> None:
        """Calculate and store final experiment statistics."""
        self.progress_tracker.log_and_callback("Calculating final statistics...", experiment.iterations, experiment.iterations)
        experiment_logger.info("Computing experiment statistics")
        
        execution_times = getattr(experiment, '_execution_times', [])
        query_plans = getattr(experiment, '_query_plans', [])
        
        if execution_times:
            experiment.avg_time = statistics.mean(execution_times)
            experiment.stddev_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0
            
            # Log statistical analysis
            experiment_logger.info(f"Average execution time: {experiment.avg_time:.4f}s")
            experiment_logger.info(f"Standard deviation: {experiment.stddev_time:.4f}s")
            experiment_logger.info(f"Min time: {min(execution_times):.4f}s")
            experiment_logger.info(f"Max time: {max(execution_times):.4f}s")
            
            # Analyze query plan changes
            if len(set(str(p) for p in query_plans)) > 1:
                experiment_logger.warning("Query plans varied between trials!")
                
        # Mark experiment as successful
        experiment.exit_status = "SUCCESS"
        experiment.experiment_logs = self.progress_tracker.get_logs_as_string()
        
        session.commit()
        session.refresh(experiment)
        
        final_msg = (
            f"Experiment completed! "
            f"Average time: {experiment.avg_time:.4f}s ± {experiment.stddev_time:.4f}s"
        )
        if execution_times:
            final_msg += f"\nMin: {min(execution_times):.4f}s, Max: {max(execution_times):.4f}s"
            
        self.progress_tracker.log_and_callback(final_msg, experiment.iterations, experiment.iterations)
        
    def _handle_experiment_failure(self, session: Session, experiment: Experiment, error: Exception) -> None:
        """Handle experiment failure and update database record."""
        session.rollback()
        error_msg = f"Experiment failed: {str(error)}"
        experiment_logger.error(error_msg)
        experiment_logger.debug(traceback.format_exc())
        
        self.progress_tracker.log_and_callback(f"❌ {error_msg}", experiment.iterations, experiment.iterations)
        
        # Update experiment with failure status
        try:
            experiment.exit_status = "FAILURE"
            experiment.experiment_logs = self.progress_tracker.get_logs_as_string()
            session.commit()
        except Exception:
            pass  # If we can't save, the main error is more important 