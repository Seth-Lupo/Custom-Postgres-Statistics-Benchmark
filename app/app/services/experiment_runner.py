"""
PostgreSQL Statistics Estimator - Experiment Runner Service

This module provides the main experiment execution orchestration functionality.
It coordinates between all the other services to execute complete experiments
while maintaining separation of concerns and modularity.

Classes:
    ExperimentRunner: Main experiment orchestration service

Author: Generated by Assistant
Created: 2024
"""

import re
import statistics
import traceback
from typing import List, Tuple, Callable, Dict, Any
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError
from sqlmodel import Session
from datetime import datetime

from ..models import Experiment, Trial
from ..src.base import StatsSource, StatsSourceConfig, StatsSourceSettings
from ..logging_config import experiment_logger, stats_logger, stats_source_logger
from ..database import create_database, drop_database, load_dump, get_db_session

from .progress_tracker import ProgressTracker
from .statistics_capture import StatisticsCapture
from .trial_executor import TrialExecutor, QueryExecutionError
from .experiment_validator import ExperimentValidator, ValidationError


class ExperimentError(Exception):
    """Base class for experiment-related errors."""
    pass


class StatsApplicationError(ExperimentError):
    """Error during statistics application."""
    pass


class ExperimentRunner:
    """
    Main experiment runner service that orchestrates experiment execution.
    
    This service coordinates:
    - Statistics source discovery and management
    - Experiment validation and setup
    - Database creation and management
    - Trial execution orchestration
    - Progress tracking and logging
    - Results aggregation and storage
    """
    
    def __init__(self):
        """Initialize the experiment runner with all necessary services."""
        self.src = {}
        self._discover_stats_sources()
        
        # Initialize services
        self.progress_tracker = ProgressTracker()
        self.statistics_capture = StatisticsCapture()
        self.trial_executor = TrialExecutor()
        self.validator = ExperimentValidator(self.src)
        
    def _discover_stats_sources(self) -> None:
        """Discover all available StatsSource subclasses."""
        for subclass in StatsSource.__subclasses__():
            # Convert class name to snake_case key
            class_name = subclass.__name__
            if class_name.endswith('StatsSource'):
                class_name = class_name[:-11]
            
            snake_case = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', class_name)
            key = re.sub('([a-z0-9])([A-Z])', r'\1_\2', snake_case).lower()
            
            self.src[key] = subclass
            
        experiment_logger.info(f"Discovered {len(self.src)} statistics sources")
    
    def get_available_src(self) -> List[Tuple[str, str]]:
        """
        Get list of available statistics sources.
        
        Returns:
            List of (key, display_name) tuples
        """
        return [(key, source().name()) for key, source in self.src.items()]
    
    def get_available_configs(self, stats_source: str) -> List[Tuple[str, str]]:
        """
        Get list of available configurations for a stats source.
        
        Args:
            stats_source: Statistics source key
            
        Returns:
            List of (config_name, display_name) tuples
        """
        if stats_source not in self.src:
            return []
        
        source_class = self.src[stats_source]
        instance = source_class()
        return instance.get_available_configs()
    
    def run_experiment(self, session: Session, stats_source: str, 
                      settings_name: str, settings_yaml: str,
                      config_name: str, config_yaml: str, query: str, iterations: int, 
                      progress_callback: Callable[[str, int, int], None], 
                      dump_path: str, name: str) -> Experiment:
        """
        Run a complete experiment and return the result.
        
        Note: Statistics reset strategy and transaction handling are now read
        from the settings YAML configuration, not passed as parameters.
        
        Args:
            session: Database session for storing results
            stats_source: Statistics source identifier
            settings_name: Named settings (optional)
            settings_yaml: Custom YAML settings (optional)
            config_name: Named configuration (optional)
            config_yaml: Custom YAML configuration (optional)
            query: SQL query to execute
            iterations: Number of trial iterations
            progress_callback: Callback function for progress updates
            dump_path: Path to database dump file
            name: Experiment name
            
        Returns:
            Completed experiment object
            
        Raises:
            ExperimentError: If experiment execution fails
            ValidationError: If parameter validation fails
        """
        db_name = "test_database"
        
        # Initialize progress tracking
        self.progress_tracker.clear_logs()
        self.progress_tracker.set_progress_callback(progress_callback)
        
        # Setup statistics source with both settings and configuration first
        # to get the strategy values from settings
        stats_source_instance, original_settings_yaml, settings_modified, settings_modified_at, original_config_yaml, config_modified, config_modified_at = \
            self._setup_stats_source_config(stats_source, settings_name, settings_yaml, config_name, config_yaml)
            
        # Get strategy values from settings
        stats_reset_strategy = stats_source_instance.settings.stats_reset_strategy
        transaction_handling = stats_source_instance.settings.transaction_handling
        
        # Validate all parameters before starting
        try:
            self.validator.validate_experiment_parameters(
                stats_source, config_name, config_yaml, query, iterations,
                stats_reset_strategy, transaction_handling, dump_path, name
            )
        except ValidationError as e:
            experiment_logger.error(f"Validation failed: {str(e)}")
            raise ExperimentError(f"Validation failed: {str(e)}") from e
        
        experiment_logger.info(f"Starting experiment '{name}' with {stats_source} stats source")
        experiment_logger.debug(f"Query: {query}")
        experiment_logger.debug(f"Iterations: {iterations}")
        experiment_logger.info(f"Settings: {settings_name or 'default'}")
        experiment_logger.info(f"Config: {config_name or 'default'}")
        experiment_logger.info(f"Stats reset strategy: {stats_reset_strategy} (from settings)")  
        experiment_logger.info(f"Transaction handling: {transaction_handling} (from settings)")
        experiment_logger.info(f"Using temporary database: {db_name} from dump: {dump_path}")
            
        # Store the actual YAML files that will be used
        actual_settings_yaml = settings_yaml if settings_yaml else original_settings_yaml
        actual_config_yaml = config_yaml if config_yaml else original_config_yaml
        
        # Create experiment record
        experiment = self._create_experiment_record(
            session, name, stats_source_instance, 
            settings_name, actual_settings_yaml, original_settings_yaml, settings_modified, settings_modified_at,
            config_name, actual_config_yaml, original_config_yaml, config_modified, config_modified_at, 
            query, iterations, stats_reset_strategy, transaction_handling
        )
        
        # Set experiment context on stats source for document association  
        stats_source_instance.set_experiment_context(experiment.id)
        
        # Execute the experiment
        try:
            self._execute_experiment_trials(
                session, experiment, stats_source_instance, query, iterations,
                stats_reset_strategy, transaction_handling, dump_path, db_name
            )
            
            # Calculate and store final statistics
            self._finalize_experiment_results(session, experiment)
            
            experiment_logger.info("Experiment completed successfully")
            return experiment
            
        except Exception as e:
            self._handle_experiment_failure(session, experiment, e)
            raise
            
    def _setup_stats_source_config(self, stats_source: str, settings_name: str, settings_yaml: str,
                                  config_name: str, config_yaml: str) -> Tuple[StatsSource, str, bool, datetime, str, bool, datetime]:
        """
        Setup statistics source instance with both settings and configuration.
        
        Returns:
            Tuple of (stats_source_instance, original_settings_yaml, settings_modified, settings_modified_at, original_config_yaml, config_modified, config_modified_at)
        """
        import yaml
        
        source_class = self.src[stats_source]
        
        # Handle settings
        settings = None
        original_settings_yaml = None
        settings_modified = False
        settings_modified_at = None
        
        if settings_yaml:
            # Use custom YAML settings provided by user
            settings_data = yaml.safe_load(settings_yaml)
            settings = StatsSourceSettings(settings_data)
            experiment_logger.info(f"Using custom settings: {settings.name}")
            
            # Get original settings for comparison
            effective_settings_name = settings_name or 'default'
            original_settings_yaml = source_class().get_settings_content(effective_settings_name)
            
            # Check if settings were actually modified
            if original_settings_yaml and settings_yaml.strip() != original_settings_yaml.strip():
                settings_modified = True
                settings_modified_at = datetime.utcnow()
                experiment_logger.info("Settings were modified from original")
            else:
                experiment_logger.info("Settings unchanged from original")
        elif settings_name:
            # Use named settings
            settings = source_class().load_settings(settings_name)
            original_settings_yaml = source_class().get_settings_content(settings_name)
            experiment_logger.info(f"Using named settings: {settings_name}")
        else:
            # Use default settings
            settings = source_class()._load_default_settings()
            original_settings_yaml = source_class().get_settings_content('default')
            experiment_logger.info("Using default settings")
        
        # Handle configuration
        config = None
        original_config_yaml = None
        config_modified = False
        config_modified_at = None
        
        if config_yaml:
            # Use custom YAML configuration provided by user
            config_data = yaml.safe_load(config_yaml)
            config = StatsSourceConfig(config_data)
            experiment_logger.info(f"Using custom configuration: {config.name}")
            
            # Get original configuration for comparison
            effective_config_name = config_name or 'default'
            original_config_yaml = source_class().get_config_content(effective_config_name)
            
            # Check if configuration was actually modified
            if original_config_yaml and config_yaml.strip() != original_config_yaml.strip():
                config_modified = True
                config_modified_at = datetime.utcnow()
                experiment_logger.info("Configuration was modified from original")
            else:
                experiment_logger.info("Configuration unchanged from original")
        else:
            # Use named or default configuration
            effective_config_name = config_name or 'default'
            config = source_class().load_config(effective_config_name)
            original_config_yaml = source_class().get_config_content(effective_config_name)
            experiment_logger.info(f"Using named/default configuration: {effective_config_name}")
        
        # Create stats source instance with both settings and config
        stats_source_instance = source_class(settings=settings, config=config)
        
        return stats_source_instance, original_settings_yaml, settings_modified, settings_modified_at, original_config_yaml, config_modified, config_modified_at
        
    def _create_experiment_record(self, session: Session, name: str, 
                                stats_source_instance: StatsSource, settings_name: str,
                                actual_settings_yaml: str, original_settings_yaml: str,
                                settings_modified: bool, settings_modified_at: datetime,
                                config_name: str, actual_config_yaml: str, original_config_yaml: str,
                                config_modified: bool, config_modified_at: datetime,
                                query: str, iterations: int, stats_reset_strategy: str,
                                transaction_handling: str) -> Experiment:
        """Create and store experiment record in database."""
        try:
            self.progress_tracker.log_and_callback("Creating experiment record...", 0, iterations)
            
            experiment = Experiment(
                name=name,
                stats_source=stats_source_instance.display_name(),
                settings_name=settings_name or 'default',
                settings_yaml=actual_settings_yaml,
                original_settings_yaml=original_settings_yaml,
                settings_modified=settings_modified,
                settings_modified_at=settings_modified_at,
                config_name=config_name or 'default',
                config_yaml=actual_config_yaml,
                original_config_yaml=original_config_yaml,
                config_modified=config_modified,
                config_modified_at=config_modified_at,
                query=query,
                iterations=iterations,
                stats_reset_strategy=stats_reset_strategy,
                transaction_handling=transaction_handling,
                exit_status="RUNNING"
            )
            
            session.add(experiment)
            session.commit()
            session.refresh(experiment)
            experiment_logger.info(f"Created experiment record with ID {experiment.id}")
            
            return experiment
            
        except SQLAlchemyError as e:
            error_msg = f"Failed to create experiment record: {str(e)}"
            experiment_logger.error(error_msg)
            experiment_logger.debug(traceback.format_exc())
            raise ExperimentError(error_msg) from e
            
    def _execute_experiment_trials(self, session: Session, experiment: Experiment,
                                 stats_source_instance: StatsSource, query: str,
                                 iterations: int, stats_reset_strategy: str,
                                 transaction_handling: str, dump_path: str, db_name: str) -> None:
        """Execute all experiment trials."""
        experiment_db_session_generator = None
        experiment_db_session = None
        
        try:
            # Setup temporary database
            self._setup_temporary_database(db_name, dump_path)
            
            # Get database session and configure it
            experiment_db_session_generator = get_db_session(db_name)
            experiment_db_session = next(experiment_db_session_generator)
            self._configure_database_session(experiment_db_session, stats_source_instance)
            
            # Apply statistics based on strategy
            if stats_reset_strategy == "once":
                self._apply_statistics_once(experiment_db_session, stats_source_instance, iterations)
                
            # Execute all trials
            self._run_all_trials(
                session, experiment, experiment_db_session, stats_source_instance,
                query, iterations, stats_reset_strategy, transaction_handling
            )
            
        finally:
            # Cleanup database resources
            if experiment_db_session_generator:
                try:
                    next(experiment_db_session_generator)
                except StopIteration:
                    pass
                    
            self.progress_tracker.log_and_callback(f"Cleaning up temporary database '{db_name}'...", iterations, iterations)
            drop_database(db_name)
            experiment_logger.info(f"Dropped temporary database: {db_name}")
            
    def _setup_temporary_database(self, db_name: str, dump_path: str) -> None:
        """Setup temporary database for experiment."""
        self.progress_tracker.log_and_callback(f"Creating temporary database '{db_name}'...", 0, 0)
        create_database(db_name)
        
        self.progress_tracker.log_and_callback(f"Loading data from '{dump_path}'...", 0, 0)
        load_dump(db_name, dump_path)
        
    def _configure_database_session(self, session: Session, stats_source_instance: StatsSource) -> None:
        """Configure database session parameters."""
        # Set session parameters for consistent execution environment using settings
        # Use default timeout if not specified (0 means no timeout)
        timeout = getattr(stats_source_instance.settings, 'analyze_timeout_seconds', 300) * 1000  # Convert to ms
        session.execute(text(f"SET statement_timeout = {timeout}"))
        
        work_mem = getattr(stats_source_instance.settings, 'work_mem', '16MB')
        session.execute(text(f"SET work_mem = '{work_mem}'"))
        
        maint_work_mem = getattr(stats_source_instance.settings, 'maintenance_work_mem', '16MB')
        session.execute(text(f"SET maintenance_work_mem = '{maint_work_mem}'"))
        
        # Use default values for settings not in the settings object
        session.execute(text("SET effective_cache_size = '1GB'"))
        session.execute(text("SET random_page_cost = 1.0"))
        session.execute(text("SET seq_page_cost = 1.0"))
        
        session.commit()
        
    def _apply_statistics_once(self, session: Session, stats_source_instance: StatsSource, iterations: int) -> None:
        """Apply statistics once before all trials."""
        msg = f"Applying {stats_source_instance.name()} statistics to database (once before all trials)..."
        self.progress_tracker.log_and_callback(msg, 0, iterations)
        experiment_logger.info(msg)
        
        try:
            stats_source_instance.apply_statistics(session)
            stats_logger.info(f"Successfully applied {stats_source_instance.name()} statistics")
        except Exception as e:
            error_msg = f"Failed to apply statistics: {str(e)}"
            stats_logger.error(error_msg)
            stats_logger.debug(traceback.format_exc())
            raise StatsApplicationError(error_msg) from e
            
    def _run_all_trials(self, session: Session, experiment: Experiment, 
                       experiment_db_session: Session, stats_source_instance: StatsSource,
                       query: str, iterations: int, stats_reset_strategy: str,
                       transaction_handling: str) -> None:
        """Execute all experiment trials."""
        execution_times = []
        query_plans = []
        
        for i in range(iterations):
            trial_msg = f"Running trial {i + 1}/{iterations}..."
            self.progress_tracker.log_and_callback(trial_msg, i, iterations)
            experiment_logger.info(trial_msg)
            
            try:
                # Apply statistics per trial if needed
                if stats_reset_strategy == "per_trial":
                    self._apply_statistics_per_trial(experiment_db_session, stats_source_instance, i + 1)
                    
                # Execute the trial
                execution_time, cost_estimate, query_plan = self.trial_executor.execute_trial(
                    experiment_db_session, query, transaction_handling, stats_source_instance
                )
                
                execution_times.append(execution_time)
                query_plans.append(query_plan)
                
                # Capture statistics snapshots
                pg_stats_snapshot, pg_statistic_snapshot = \
                    self.statistics_capture.capture_statistics_snapshots(experiment_db_session)
                
                # Record trial in database
                self._record_trial_result(
                    session, experiment.id, i + 1, execution_time, cost_estimate,
                    pg_stats_snapshot, pg_statistic_snapshot, query_plan
                )
                
                result_msg = f"Trial {i + 1} completed: Time={execution_time:.4f}s, Cost={cost_estimate:.2f}"
                self.progress_tracker.log_and_callback(result_msg, i + 1, iterations)
                
            except Exception as e:
                error_msg = f"Error in trial {i + 1}: {str(e)}"
                experiment_logger.error(error_msg)
                self.progress_tracker.log_and_callback(f"⚠️ {error_msg}", i + 1, iterations)
                raise QueryExecutionError(error_msg) from e
                
        # Store execution times for final analysis
        experiment._execution_times = execution_times
        experiment._query_plans = query_plans
        
    def _apply_statistics_per_trial(self, session: Session, stats_source_instance: StatsSource, trial_num: int) -> None:
        """Apply statistics for a specific trial."""
        trial_stats_msg = f"Resetting and applying {stats_source_instance.name()} statistics for trial {trial_num}..."
        self.progress_tracker.log_and_callback(trial_stats_msg, trial_num - 1, 0)
        experiment_logger.info(trial_stats_msg)
        
        try:
            stats_source_instance.apply_statistics(session)
            stats_logger.info(f"Successfully applied {stats_source_instance.name()} statistics for trial {trial_num}")
        except Exception as e:
            error_msg = f"Failed to apply statistics for trial {trial_num}: {str(e)}"
            stats_logger.error(error_msg)
            stats_logger.debug(traceback.format_exc())
            raise StatsApplicationError(error_msg) from e
            
    def _record_trial_result(self, session: Session, experiment_id: int, run_index: int,
                           execution_time: float, cost_estimate: float,
                           pg_stats_snapshot: str, pg_statistic_snapshot: str, query_plan: Dict[str, Any]) -> None:
        """Record trial result in database."""
        import json
        
        trial = Trial(
            experiment_id=experiment_id,
            run_index=run_index,
            execution_time=execution_time,
            cost_estimate=cost_estimate,
            pg_stats_snapshot=pg_stats_snapshot,
            pg_statistic_snapshot=pg_statistic_snapshot,
            query_plan=json.dumps(query_plan, default=str, indent=2)
        )
        session.add(trial)
        session.commit()
        
    def _finalize_experiment_results(self, session: Session, experiment: Experiment) -> None:
        """Calculate and store final experiment statistics."""
        self.progress_tracker.log_and_callback("Calculating final statistics...", experiment.iterations, experiment.iterations)
        experiment_logger.info("Computing experiment statistics")
        
        execution_times = getattr(experiment, '_execution_times', [])
        query_plans = getattr(experiment, '_query_plans', [])
        
        if execution_times:
            experiment.avg_time = statistics.mean(execution_times)
            experiment.stddev_time = statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0
            
            # Log statistical analysis
            experiment_logger.info(f"Average execution time: {experiment.avg_time:.4f}s")
            experiment_logger.info(f"Standard deviation: {experiment.stddev_time:.4f}s")
            experiment_logger.info(f"Min time: {min(execution_times):.4f}s")
            experiment_logger.info(f"Max time: {max(execution_times):.4f}s")
            
            # Analyze query plan changes
            if len(set(str(p) for p in query_plans)) > 1:
                experiment_logger.warning("Query plans varied between trials!")
                
        # Mark experiment as successful
        experiment.exit_status = "SUCCESS"
        experiment.experiment_logs = self.progress_tracker.get_logs_as_string()
        
        session.commit()
        session.refresh(experiment)
        
        final_msg = (
            f"Experiment completed! "
            f"Average time: {experiment.avg_time:.4f}s ± {experiment.stddev_time:.4f}s"
        )
        if execution_times:
            final_msg += f"\nMin: {min(execution_times):.4f}s, Max: {max(execution_times):.4f}s"
            
        self.progress_tracker.log_and_callback(final_msg, experiment.iterations, experiment.iterations)
        
    def _handle_experiment_failure(self, session: Session, experiment: Experiment, error: Exception) -> None:
        """Handle experiment failure and update database record."""
        session.rollback()
        error_msg = f"Experiment failed: {str(error)}"
        experiment_logger.error(error_msg)
        experiment_logger.debug(traceback.format_exc())
        
        self.progress_tracker.log_and_callback(f"❌ {error_msg}", experiment.iterations, experiment.iterations)
        
        # Update experiment with failure status
        try:
            experiment.exit_status = "FAILURE"
            experiment.experiment_logs = self.progress_tracker.get_logs_as_string()
            session.commit()
        except Exception:
            pass  # If we can't save, the main error is more important 